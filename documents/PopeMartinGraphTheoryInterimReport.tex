\documentclass{article}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\usepackage{graphicx}

\begin{document}

\title{Evolving Random Graph Generating Algorithms}

\author{
Pope, Aaron\\
\texttt{aaron.pope@mst.edu}
\and
Martin, Matthew\\
\texttt{mam446@mst.edu}
}

\maketitle

\section{Introduction}

\subsection{Random Graphs}

Random graphs have a variety of applications in graph theory. The graphs themselves can be described as a model consisting of a probability distribution of degrees, adjacency or some other characteristic. Alternatively, random graphs can be described by a stochastic process of generating a graph within certain constraints. The research presented in this work will focus on this latter approach.

How well a random graph model describes a given graph can be determined by performing a statistical goodness of fit test. Inversely, if a random process is to produce graphs which fit well with a given random graph model, a goodness of fit test performed on a population of resulting graphs with respect to the desired model can be used to determine how well the process produces graphs described by the model. 

It is possible to have desirable features within a graph that are not captured or guaranteed by a random graph model. An example of this would be the occurrence of certain topology motifs within the graph. The presence of these motifs can have a large impact on characteristics of the graph as a whole, such as the resilience or robustness of the graph. These motifs, along with other features, can be detected in the graphs which are generated by a given process. The quality of the graph generating process can be determined by how consistently it produces graphs with a high occurrence of desirable features and a low occurrence of undesirable features.

\subsection{Evolutionary Computing and Genetic Programming}

Evolutionary Computing (EC) uses a biologically inspired process to solve a problem by generating a population of potential solutions and selecting the best among them to participate in procreation to create more solutions. This process continues through multiple generations until certain termination criteria are met. Genetic Programming (GP) is a field of EC in which the solutions sought are programs themselves. The candidate programs can be represented as parse trees in which the internal nodes represent operations on the input received from the children nodes and the leaf nodes are chosen from the problem's input.

Traditional GP has several stochastic components. The population is initialized with randomly created solutions, offspring inherit a random selection of attributes from each parent and each offspring has a chance of undergoing a random mutation. Because of these stochastic elements, a GP algorithm will explore possible solutions that might seem counterintuitive to a human developer. While these non-obvious choices are usually inferior, they can, on occasion, lead to a breakthrough in computational efficiency or solution accuracy.

A major challenge in implementing a GP algorithm is creating a set of operations which can be combined to create a candidate solution. The operation set needs to contain a variety of methods in order for the GP to create solutions to a variety of problem types. However, if the set grows too large, the search space of potential solutions explodes, making a search infeasible. Therefore, the goal is to find a minimal set of operations which are crucial to solving a specific problem or family of problems.

Another challenge is measuring the quality of a candidate solution when determining which solutions will survive and procreate. In EC, this measure is known as the fitness of the solution. Often the process of evaluating the fitness of individual solutions in the population is the performance bottleneck of the evolutionary algorithm. For this reason, it is desirable to have an efficient method of evaluating a solution's quality. Even if no such efficient evaluation method is available, using genetic programming can still be beneficial. If the GP algorithm seeks to replace a program which is more efficient than another which is commonly used, then the time to find a new highly specialized solution can be justified.

If an optimal fitness value is known, potential solutions can be compared to this ideal value to determine their quality. If an optimal solution value is not known, the individuals in the population can be compared to one another to determine relative quality. To evaluate the quality of a GP solution, the parse tree representation is converted to a program which is run with a set of various input samples. The accuracy of the program's output is used to measure the quality of the solution.

The method of evaluating random graph generating programs will depend on the type of problem the GP used to solve. If a program is sought which produces graphs that adhere to a specific random graph model, then the program can be used to generate a set of graphs which are evaluated using a goodness of fit test to determine how well they fit the desired model. If the goal is instead to generate graphs which exhibit some feature, the program can be evaluated by determining the rate of occurrence of these features in the graphs it produces. Programs produce graphs that adhere better to the intended model or have a higher rate of occurrence for desirable features will have higher relative fitness values.

\section{Related Work}

Random graphs and random graph models have been studied extensively in various applications
in previous research. Some simple models use independent probabilities for the presence of
each edge~\cite{Erdos:59,Erdos:60}. Exponential random graph models use vertex degree
distributions which have been found to resemble the structure of social and other types of
networks~\cite{Wasserman:94,Robins:07}. Wireless sensor networks have been modeled as random
geometric graphs which are created by placing vertices randomly within an area and connecting
vertices within a certain radius of each other~\cite{Avin:07,Diaz:01}.
 
There have been many works done on evolving algorithms to do a specific 
task~\cite{Lourenco:12,Martin:13}. However, there have been no applications of evolving
random graph generating algorithms or graph algorithms in general. Though the application of 
the work in evolving algorithms is different from the goals of this paper, the concepts that
those algorithms are based on can be used in this application. These papers use Koza style 
GP~\cite{Koza:92} and Grammatical Evolution~\cite{Ryan:98}. In this paper, Koza
style GP will be used to construct the random graph generating algorithm. 

\section{Motivation}

The goal of this research is to develop a general purpose genetic algorithm which evolves programs that generate random graphs that satisfy certain constraints. This would provide a source of material that could be used to facilitate further research in various areas of graph theory. For instance, if researchers want to examine the efficiency of a new networking algorithm when applied to scale free networks, a program which generates power law distribution graphs can be evolved and used to generate test graphs. The versatility of the genetic algorithm approach will allow this to be done even when the desired features of a random graph are difficult to express within a random graph model.

\section{Methodology}

\begin{figure}
\begin{centering}
  \includegraphics[scale=0.4]{RandomGraphExample2.png}
  \caption{Example Random Graph Generating Parse Tree}
  \label{fig:example}
\end{centering}
\end{figure}


This paper discusses a method using GP to evolve an algorithm that generates a random graph 
of with a specified nature.  The method proposed uses Koza style GP to evolve a post-ordered 
parse tree structure that will represent the algorithm. Most random-graph generating algorithms 
repeat a node adding phase $n$ times to create a random graph of size $n$. Our algorithm takes 
advantage of this repetitive nature of random-graph algorithms in such a way that it only 
evolves the method for adding a vertex and its connections. In this phase, the parse tree decides 
which vertices will be adjacent to the new vertex.


\subsection{Parse-Tree}
There are two proposed methods that this decision process can be done. One method is for each new vertex, 
the algorithm would iterate over every existing vertex and the parse tree would determine if a 
connection would be made between that vertex and the new vertex. The second method would be for the parse 
tree to be evaluated one time and determine the set of vertices to which the new node will be connected.  
It was determined that the second method would be the preferred option, as it would be easier to 
construct some of the pre-existing random graph algorithms such as the Erd\H{o}s-R\'{e}nyi model.

\subsection{Nodes}
The parse tree will be constructed from a set of operations that will be used as the non-terminal 
nodes. Sets of vertices which will be used as the terminal nodes in the parse tree.  Each of the operations will take 
in one or more sets of vertices and return a single set of vertices. The set of vertices returned by the 
root node will be the set of vertices that the newly created vertex will connect to. 
An example parse tree can be seen in Figure \ref{fig:example}.

There are two primary types of non-terminal operations that are used as nodes in the construction of the parse-tree. The
first of which are selection operations. These operations take a set of vertices and by some method selects vertices from
that set and returns them to its parent node. The second type of non-terminal operations are set operations. These nodes 
typically take two sets of vertices and perform some kind of set operation on the two sets and return the product to its
parent node. The exact specifications of the operations that were used as nodes can be found below.

\subsubsection{Selection Nodes}
There are two types of selection nodes that were used to evolve the Random Graph Algorithms. The first of which are standard
selection operations that have constant values for their parameters. The second are relative selection operations. For this 
second type of selection operations, the parameter values used are relative to the size of the graph. This addition was made
so that the Random Grpah Algorithms would create graphs with the same properties as the size of the graph increased substantially. 

In a majority of the selection operations, the vertices are compared to one another to determine selection. When this is the case, 
they are compared based upon some measure of the node. In the experiments reported in this paper, the only measure that was available
was the degree of the vertex. This however could be any measure of a node such as its eccentricity or other measures. For these
selection operations that are based on vertex measures, the selection can be done as a minimization or maximization of that measure. For
example, the operation could try to select vertices with a high degree or select vertices with a low degree.

For the first type of selection operation, there were four selection operations that were used. The first of which is call `p-Select'.
This selection operation iterates over the set of vertices that were passed in and has a $p$ probability of selecting each vertex. The
second selection operation is the standard k-Tournament selection with replacement. This operation has two parameters; one of which is $k$ which
is the number of vertices to compare each iteration, and $num$ which is the number of vertices selected and passed to its parent node. Note that 
vertices can be reselected during the selection process, they will not appear multiple times in the sets that are passed to its parent. This 
is done to insure that the resulting random graphs are simple. The third selection operation is a random sub-set operation. This operation takes 
a random sub-set of size $n$ from the set passed and returns that set to its parent node. This selection is done without replacement. The fourth 
selection operation is truncation selection. This operation selects the highest or lowest $n$ vertices based on the measure specified, and passes
them to its parent node.

The second type of selelction operation is relative selections. There are three selection operations of this type used in the experiments presented
in this paper. They are the k-Tournament selection, random sub-set selection and truncation selection that were presented earlier. The difference
in this version of the selection is that each of the parameters of the selection are relative to the size of the graph. An example of the
relation for k-Tournament's $k$ can be seen in Equation~\ref{eq:example}.

\begin{equation}
\label{eq:example}
k_{val} = \floor*{\frac{n}{k_{rel}}}
\end{equation}

In the above equation, $k_{val}$ is the $k$ value that is used for that run of the k-Tournament function, $n$ is the number of vertices in the graph, and $k_{rel}$
is a constant parameter value that is associated with that k-Tournament node.


\subsubsection{Set Nodes}

The second type of operation that is used in the presented experiments are set operations. All of these operations take two sets of vertices and return
a single set of vertices to its parent node. There are four set operations that are used in the experiments. The first is the union operation which returns
the vertices which are in both sets passed in. The second is the intersection which returns the vertices which are in common between the two sets passed in. 
The third is the difference operation which returns the vertices that are in the left child's set but not in the right child's. The fourth is the symmetric
difference operation which returns the vertices that are in either the left child's set or the right child's set but not in both. 


\subsection{Meta-Algorithm}
GP is employed to meta-evolve the Random Graph Algorithms. The two primary variation operators employed are the standard sub-tree crossover and the sub-tree
mutation operator altered to make the maximum number of nodes being added a user defined value. Another mutation operation was added to this algorithm that 
selects a node at random from the parse tree and randomizes the parameters if it has any; if the node does not have any parameters the mutation is executed again. 
The parameters are set to a random valued within a specified user range for each nodes parameters. This can also alter the type of measure used in the selection
operations and if the selections attempt to minimize or maximize the measure. 

\subsubsection{Random Graph Algorithm}
Each individual in the GP population encodes a Random Graph Algorithm. To evaluate the fitness of an individual its encoded Random Graph Algorithm generates
a random graph a user-defind number of times. The resulting graphs are generated to be the same size to ensure an accurate fitness. These graphs are evaluated 
using a user defined fitness function. The fitness of the Random Graph Algorithm is the average fitness of the graphs that it produces. 

The graphs are generated by iteratively executing the parse-tree. Each iteration a single vertex is added to the graph and the parse tree is executed to determine
which vertices the new vertex will share an edge with. By employing this method, every possible graph is possible to be generated assuming that the right operations 
are available for the evolution process. 

\subsubsection{Fitness Calculation}
For the experiments presented in this paper, there were many different fitness functions employed to demonstrate the capabilities of the algorithm that
is proposed. The primary graph characteristics that were employed for fitness functions were graph partitioning, eccentricity, and the number of edges.  
Four fitness functions were constructed from these graph characteristics and using during evolution. For each of the functions, it was required that the graph
be connected. If the graph was not connected the fitness value was set to an arbitrarily small value.

The first fitness function was designed to maximize 
the edgecut needed to partition the graph into two pieces while minimizing the number of edges in the graph. To approximate the edgecut of the partition,
the Metis algorithm was used. The edges are minimized to prevent a complete graph from being evolved. This fitness function can be seen in Equation~\ref{eq:maxMetis}.

\begin{equation}
\label{eq:maxMetis}
fitness = c_1E_c - c_2E
\end{equation}

In the above equation, $c_1$ and $c_2$ are weighting constants, $E_c$ is the edge cut of the graph, and $E$ is the number of edges in the graph.


The second fitness function was designed to minimize 
the edgecut needed to partition the graph into two pieces while maximizing the number of edges in the graph. To approximate the edgecut of the partition,
the Metis algorithm was used. The edges are maximized to prevent an empty graph from being evolved. This fitness function can be seen in Equation~\ref{eq:maxMetis}.

\begin{equation}
\label{eq:maxMetis}
fitness = -c_1E_c + c_2E
\end{equation}

In the above equation, $c_1$ and $c_2$ are weighting constants, $E_c$ is the edge cut of the graph, and $E$ is the number of edges in the graph.









\subsubsection{External Verification}
To assure that the performance of the evolved Random Graph Algorithms areconsistent with their performance reported during evolution,
executable code is generated to represent the parse tree. This is done to externally verify that the performance that the GP shows for a
given Random Graph Algorithm is accurate when actually implemented. The generated code is used in all of the experiments to ensure unbiased execution 
of the Random Graph Algorithms.


\section{Experimental Setup}

\section{Results}

\section{Discussion}

\section{Conclusion}

\section{Future Work}

%The main nodes that will be used as the non-terminal nodes will be selection nodes. These nodes will take a set of nodes and select a subset of those nodes to return to its parent node. One example is the selection criterion that is used in the Erd\H{o}s-R\'{e}nyi model. Every node has a $p$ percent chance of getting selected.  Other selection operations such as k-tournament, proportional selection and others can also be used if there is some measure of the nodes that they can be selected on. This could be the degree of the node or some other property that can be attributed to that node.  Other nodes that will be included are set operation nodes such as union, intersection, and disjoint.  By including selection operations that are taken from current random graph models we are guaranteed to be able to represent those models in the parse tree structure.

%To evaluate these parse trees random graphs will be generated using the method described in the parse tree. The parse trees will be given a fitness value based on how closely the graphs they generate comply with a user defined model. For the experiments that will be constructed, the models will be fairly rudimentary to show a proof of concept. An example of this could be all nodes have an equal or near equal degree and the degree is roughly $\frac{n}{3}$, $n$ being the number of nodes in the graph.  The user can specify the model and the standard deviation away from the model that the random graphs may have. For the fitness function, $k$ graphs at evenly spaced intervals between the min and max graph size of interest will be sampled. The difference at each point will be averaged along with the standard deviation used in a weighted sum to give each parse tree a fitness value.

%The GP will be using standard GP operators along with other operators to help with a local search of the parse trees. The standard GP operators include subtree crossover and subtree mutation. The new operator is an alternate mutation operator which randomly selects a node of the parse tree and randomly changes the parameters of the operation contained in that node.



\bibliographystyle{abbrv}
\bibliography{sigproc} 


\end{document}


